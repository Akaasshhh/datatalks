# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KmQyoo_w_q63mxk7oGFksIh2rJUhNxjA
"""

!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/car_fuel_efficiency.csv

import pandas as pd

df = pd.read_csv('car_fuel_efficiency.csv')

df.isnull().sum()

df.dtypes

df = df.fillna(0)

df.isnull().sum()

df.head()

for i in df.columns:
  print(df[i].value_counts())

import numpy as np

df['num_doors'] = np.abs(df['num_doors'])

df['num_doors'].value_counts()

df.describe()

from sklearn.model_selection import train_test_split

df_full_train,df_test = train_test_split(df,test_size=0.2,random_state=1)
df_train,df_val = train_test_split(df_full_train,test_size=0.2,random_state=1)

df_train = df_train.reset_index(drop=True)
df_val = df_val.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)

y_train = df_train['fuel_efficiency_mpg']
y_val = df_val['fuel_efficiency_mpg']
y_test = df_test['fuel_efficiency_mpg']

del df_train['fuel_efficiency_mpg']
del df_val['fuel_efficiency_mpg']
del df_test['fuel_efficiency_mpg']

df_train

from sklearn.tree import DecisionTreeRegressor

from sklearn.feature_extraction import DictVectorizer

dv = DictVectorizer(sparse=True)

dv

train_dicts = df_train.to_dict(orient='records')
X_train = dv.fit_transform(train_dicts)

X_train

dv.get_feature_names_out()

dt = DecisionTreeRegressor(max_depth=10)

dt.fit(X_train,y_train)

from sklearn.tree import export_text

print(export_text(dt,feature_names=dv.get_feature_names_out()))

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=10,random_state=1,n_jobs=-1)

rf.fit(X_train,y_train)

X_val = dv.transform(df_val.to_dict(orient='records'))

y_pred_val = rf.predict(X_val)

from sklearn.metrics import r2_score

r2_score(y_val,y_pred_val)

scores = []
for n in range(10,201,10):
  rf = RandomForestRegressor(n_estimators=n,random_state=1,n_jobs=-1)
  rf.fit(X_train,y_train)
  y_pred_val = rf.predict(X_val)
  scores.append((n,r2_score(y_val,y_pred_val)))

scores = pd.DataFrame(scores,columns=['n_estimators','r2_score'])

import matplotlib.pyplot as plt

plt.plot(scores['n_estimators'],scores['r2_score'])

scores = []
for d in [10,15,20,25]:
  for n in range(10,201,10):
    rf = RandomForestRegressor(n_estimators=n,random_state=1,n_jobs=-1)
    rf.fit(X_train,y_train)
    y_pred_val = rf.predict(X_val)
    scores.append((d,n,r2_score(y_val,y_pred_val)))

df_scores = pd.DataFrame(scores,columns=['max_depth','n_estimators','r2_score'])

for i in [10,15,20,25]:
  df_subset = df_scores[df_scores['max_depth']==i]
  plt.plot(df_subset['n_estimators'],df_subset['r2_score'],label='max_depth='+str(i))
plt.legend()

df_scores

rf = RandomForestRegressor(n_estimators=10,
max_depth=20,
random_state=1,
n_jobs=-1)

rf.fit(X_train,y_train)

feature = pd.Series(rf.feature_importances_,index=dv.get_feature_names_out())

feature

import xgboost as xgb

features = dv.get_feature_names_out().tolist()

dtrain = xgb.DMatrix(X_train,label=y_train,feature_names=features)
dval = xgb.DMatrix(X_val,label=y_val,feature_names=features)



xgb_params = {
    'eta': 0.3,
    'max_depth': 6,
    'min_child_weight': 1,

    'objective': 'reg:squarederror',
    'nthread': 8,

    'seed': 1,
    'verbosity': 1,
}

model = xgb.train(xgb_params,dtrain,num_boost_round=100)

model.predict(dval)

r2_score(y_val,model.predict(dval))

xgb_params = {
    'eta': 0.1,
    'max_depth': 6,
    'min_child_weight': 1,

    'objective': 'reg:squarederror',
    'nthread': 8,

    'seed': 1,
    'verbosity': 1,
}

model = xgb.train(xgb_params,dtrain,num_boost_round=100)

r2_score(y_val,model.predict(dval))

